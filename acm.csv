,title,year,doi,abstract
0,HAID: Human-AI Interaction for Dementia Care,2024,10.1145/3677045.3685469,
1,Theory of Mind in Human-AI Interaction,2024,10.1145/3613905.3636308,"Theory of Mind (ToM), humans’ capability of attributing mental states such as intentions, goals, emotions, and beliefs to ourselves and others, has become a concept of great interest in human-AI interaction research. Given the fundamental role of ToM in human social interactions, many researchers have been working on methods and techniques to equip AI with an equivalent of human ToM capability to build highly socially intelligent AI. Another line of research on ToM in human-AI interaction seeks to understand people’s tendency to attribute mental states such as blame, emotions, and intentions to AI, along with the role that AI should play in the interaction (e.g. as a tool, partner, teacher, facilitator, and more) to align with peoples’ expectations and mental models. The goal of this line of work is to distill human-centered design implications to support the development of increasingly advanced AI systems. Together, these two research perspectives on ToM form an emerging paradigm of “Mutual Theory of Mind (MToM)” in human-AI interaction, where both the human and the AI each possess the ToM capability. This workshop aims to bring together different research perspectives on ToM in human-AI interaction by engaging with researchers from various disciplines including AI, HCI, Cognitive Science, Psychology, Robotics, and more to synthesize existing research perspectives, techniques, and knowledge on ToM in human-AI interaction, as well as envisioning and setting a research agenda for MToM in human-AI interaction."
2,Assessing Human-AI Interaction Early through Factorial Surveys: A Study on the Guidelines for Human-AI Interaction,2022,10.1145/3511605,"This work contributes a research protocol for evaluating human-AI interaction in the context of specific AI products. The research protocol enables UX and HCI researchers to assess different human-AI interaction solutions and validate design decisions before investing in engineering. We present a detailed account of the research protocol and demonstrate its use by employing it to study an existing set of human-AI interaction guidelines. We used factorial surveys with a 2 × 2 mixed design to compare user perceptions when a guideline is applied versus violated, under conditions of optimal versus sub-optimal AI performance. The results provided both qualitative and quantitative insights into the UX impact of each guideline. These insights can support creators of user-facing AI systems in their nuanced prioritization and application of the guidelines."
3,Spiritual AI: Exploring the Possibilities of a Human-AI Interaction Beyond Productive Goals,2024,10.1145/3613905.3650743,"The Human-Computer Interaction community has long endeavored to discuss technologies that go beyond productivity goals. We extend this perspective to the realm of Human-AI interaction to explore how AI could consider diverse user values, particularly in users’ prayer experiences where minimal productivity goals exist. Through diary study, our research identified user goals and behaviors that contribute to satisfying prayer experiences. Then, we conceptualized four distinct AI systems designed to celebrate the identified goals and behaviors of the users. We presented these conceptualized systems in the format of a design workbook and engaged users in evaluating them. Based on our findings, we discuss the potential of novel roles that AI could play in human lives, such as provoking deep reflections or creating indirect communities."
4,Development and translation of human-AI interaction models into working prototypes for clinical decision-making,2024,10.1145/3643834.3660697,"In the standard interaction model of clinical decision support systems, the system makes a recommendation, and the clinician decides whether to act on it. However, this model can compromise the patient-centeredness of care and the level of clinician involvement. There is scope to develop alternative interaction models, but we need methods for exploring and comparing these to assess how they may impact clinical decision-making. Through collaborating with clinical, AI safety, and HCI experts, and patient representatives, we co-designed a number of alternative human-AI interaction models for clinical decision-making. We then translated these models into ‘Wizard of Oz’ prototypes, where we created clinical scenarios and designed user interfaces with different types of AI output. In this paper, we present alternative models of human-AI interaction and illustrate how we used a co-design approach to translate them into functional prototypes that can be tested with users to explore potential impacts on clinical decision-making."
5,Exploration of Explainable AI for Trust Development on Human-AI Interaction,2023,10.1145/3639592.3639625,"In recent years, the revolutionary impact of Artificial Intelligence (AI) cannot be overstated. This groundbreaking technology has radically transformed how we perform our daily tasks, thereby redefining the very fabric of our society. However, as our reliance on AI systems continues to grow, the need for calibrated trust becomes increasingly pressing. To address this concern, the concept of Explainable AI (XAI) has been introduced to provide human-level explanations. The primary goal is to offer cognitive information that prompts informed trust decisions. Nevertheless, it is essential to recognize that trust is a multidimensional construct, involving various means of processing beyond mere explanations. To fully understand and explore these dimensions within the context of XAI, this research aims to uncover and comprehend the additional facets of trust. Through an exploratory survey, it was confirmed that XAI serves a vital purpose in facilitating trust, and it can be effectively processed through affective means. Furthermore, the presentation of information beyond the depth of explanation was found to play a significant role in moderating trust formation during human-AI interactions."
6,Re-examining User Burden in Human-AI Interaction: Focusing on a Domain-Specific Approach,2024,10.1145/3613905.3638186,"In my thesis, I revisit, specialize, and expand the concept of ‘user burden’ in three different contexts. In the first study, I explore what user burdens occur when deleting unused apps. To do so, I have conducted in-depth interviews, designed questionnaires, and performed scenario-based experiments. In my second study, I designed and developed a conversational agent that documents and reports cases of sexual assault survivors to the police on behalf of the survivors. To discover survivors’ burdens and find solutions to mitigate them, I conducted in-depth interviews and participatory design sessions with sexual assault survivors, as well as diverse stakeholders (e.g., police officers, counselors). In my third study, I investigated why employees resist algorithmic evaluations in workplaces and how to mitigate these burdens. The goal of participating in this doctoral consortium is to share the three lines of research for my thesis with researchers and professors and gain diverse ideas and feedback from the HCI community to better synthesize my works."
7,Past Meets Future: Human-AI Interaction for Digital History and Cultural Heritage,2024,10.1145/3640544.3645257,"Digital History and Cultural Heritage encapsulate invaluable societal narratives, yet scholars and practitioners face challenges in data quality, accessibility, and engagement. Human-AI Interaction (HAI) holds promise to address these challenges, fostering enhanced analysis, discoverability, and storytelling at scale. However, its potential remains largely untapped by the HAI community. This workshop aimed to bridge this gap, inviting inviting scholars and practitioners from fields such as human-computer interaction (HCI), artificial intelligence (AI), history, cultural heritage, and GLAMs (galleries, libraries, archives, and museums) to explore innovative HAI methodologies and frameworks tailored to these domains. Through interdisciplinary dialogue, we aimed to propose tractable solutions, enriching both the Digital History and Cultural Heritage sectors, as well as the HAI field, while nurturing a fertile ground for historical storytelling and meaningful engagement with our shared past."
8,"A Systematic Review on Fostering Appropriate Trust in Human-AI Interaction: Trends, Opportunities and Challenges",2024,10.1145/3696449,"Appropriate Trust in Artificial Intelligence (AI) systems has rapidly become an important area of focus for both researchers and practitioners. Various approaches have been used to achieve it, such as confidence scores, explanations, trustworthiness cues, or uncertainty communication. However, a comprehensive understanding of the field is lacking due to the diversity of perspectives arising from various backgrounds that influence it and the lack of a single definition for appropriate trust. To investigate this topic, this paper presents a systematic review to identify current practices in building appropriate trust, different ways to measure it, types of tasks used, and potential challenges associated with it. We also propose a Belief, Intentions, and Actions (BIA) mapping to study commonalities and differences in the concepts related to appropriate trust by (a) describing the existing disagreements on defining appropriate trust, and (b) providing an overview of the concepts and definitions related to appropriate trust in AI from the existing literature. Finally, the challenges identified in studying appropriate trust are discussed, and observations are summarized as current trends, potential gaps, and research opportunities for future work. Overall, the paper provides insights into the complex concept of appropriate trust in human-AI interaction and presents research opportunities to advance our understanding on this topic."
9,"Human-AI interaction: intermittent, continuous, and proactive",2021,10.1145/3486941,"user interaction with AI systems does not necessarily follow this rigid interaction pattern. Inspired by Kristina Höök and Yang et al. [1,2], we define human-AI interaction as the completion of a user’s task with the help of AI support, which may manifest itself in non-intermittent scenarios. By overlooking these other interaction paradigms, we neglect the opportunity to define and support alternative human-AI scenarios. In this article, we present and outline three types of human-AI interaction paradigms, which we refer to as intermittent, continuous, and proactive, highlighting a diverse set With the rise in artificial intelligence (AI)–driven interactive systems, both academics and practitioners within human-computer interaction (HCI) have a growing focus on human-AI interaction. This has resulted in, for example, system-design guidelines and reflections on the differences and challenges when designing for AI-driven interaction as opposed to moretraditional applications [1]. We argue that the current work on human-AI interaction is defined primarily by a focus on what we refer to as intermittent interaction scenarios, in which there is a clear line between the human initiator of an interaction and an almost W Insights → We have identified three types of human-AI interaction paradigms, which we refer to as intermittent, continuous, and proactive. → HCI researchers and practitioners need to systematically explore novel techniques to support end users in non-intermittent AI interaction scenarios. Human-AI Interaction: Intermittent, Continuous, and Proactive Niels van Berkel, Mikael B. Skov, and Jesper Kjeldskov, Aalborg University"
10,Measuring User Experience Inclusivity in Human-AI Interaction via Five User Problem-Solving Styles,2021,10.1145/3663740,"Motivations: Recent research has emerged on generally how to improve AI products’ Human-AI Interaction (HAI) User Experience (UX), but relatively little is known about HAI-UX inclusivity. For example, what kinds of users are supported, and who are left out? What product changes would make it more inclusive? Objectives: To help fill this gap, we present an approach to measuring what kinds of diverse users an AI product leaves out and how to act upon that knowledge. To bring actionability to the results, the approach focuses on users’ problem-solving diversity. Thus, our specific objectives were: (1) to show how the measure can reveal which participants with diverse problem-solving styles were left behind in a set of AI products; and (2) to relate participants’ problem-solving diversity to their demographic diversity, specifically gender and age. Methods: We performed 18 experiments, discarding two that failed manipulation checks. Each experiment was a 2x2 factorial experiment with online participants, comparing two AI products: one deliberately violating one of 18 HAI guideline and the other applying the same guideline. For our first objective, we used our measure to analyze how much each AI product gained/lost HAI-UX inclusivity compared to its counterpart, where inclusivity meant supportiveness to participants with particular problem-solving styles. For our second objective, we analyzed how participants’ problem-solving styles aligned with their gender identities and ages. Results & Implications: Participants’ diverse problem-solving styles revealed six types of inclusivity results: (1) the AI products that followed an HAI guideline were almost always more inclusive across diversity of problem-solving styles than the products that did not follow that guideline—but “who” got most of the inclusivity varied widely by guideline and by problem-solving style; (2) when an AI product had risk implications, four variables’ values varied in tandem: participants’ feelings of control, their (lack of) suspicion, their trust in the product, and their certainty while using the product; (3) the more control an AI product offered users, the more inclusive it was; (4) whether an AI product was learning from “my” data or other people’s affected how inclusive that product was; (5) participants’ problem-solving styles skewed differently by gender and age group; and (6) almost all of the results suggested actions that HAI practitioners could take to improve their products’ inclusivity further. Together, these results suggest that a key to improving the demographic inclusivity of an AI product (e.g., across a wide range of genders, ages, etc.) can often be obtained by improving the product’s support of diverse problem-solving styles."
11,"Designing for Human-AI Interaction: Comparing Intermittent, Continuous, and Proactive Interactions for a Music Application",2024,10.1145/3613905.3650886,"Designing effective and user-centred interactions between humans and AI systems poses fundamental challenges. The behaviour of AI systems is complex and uncertain, making it difficult to envision and craft optimal user experiences. Improved frameworks are needed to guide the design of human-AI interaction. In this paper, we develop and evaluate prototypes for a music application, representing three distinct paradigms of human-AI interaction: Intermittent, Continuous, and Proactive. Through qualitative user interviews with 12 participants, we compare the user experience across these prototypes, shedding light on potential challenges and opportunities for the paradigms represented. We found that the three prototypes exhibit distinct characteristics in terms of supported goals and user control. This case study contributes to a deeper understanding of the complexities involved in designing AI systems and offers insights for the development of more user-centred AI applications."
12,"""Help Me Help the AI"": Understanding How Explainability Can Support Human-AI Interaction",2022,10.1145/3544548.3581001,"Despite the proliferation of explainable AI (XAI) methods, little is understood about end-users’ explainability needs and behaviors around XAI explanations. To address this gap and contribute to understanding how explainability can support human-AI interaction, we conducted a mixed-methods study with 20 end-users of a real-world AI application, the Merlin bird identification app, and inquired about their XAI needs, uses, and perceptions. We found that participants desire practically useful information that can improve their collaboration with the AI, more so than technical system details. Relatedly, participants intended to use XAI explanations for various purposes beyond understanding the AI’s outputs: calibrating trust, improving their task skills, changing their behavior to supply better inputs to the AI, and giving constructive feedback to developers. Finally, among existing XAI approaches, participants preferred part-based explanations that resemble human reasoning and explanations. We discuss the implications of our findings and provide recommendations for future XAI design."
13,Evaluating Interactive AI: Understanding and Controlling Placebo Effects in Human-AI Interaction,2024,10.1145/3613905.3636304,"In the medical field, patients often experience tangible benefits from treatments they expect will improve their condition, even if the treatment has no mechanism of effect. This phenomenon often obscuring scientific evaluation of human treatment is termed the ""placebo effect."" Latest research in human-computer interaction has shown that using cutting-edge technologies similarly raises expectations of improvement, culminating in placebo effects that undermine evaluation efforts for user studies. This workshop delves into the role of placebo effects in human-computer interaction for cutting-edge technologies such as artificial intelligence, its influence as a confounding factor in user studies, and identifies methods that researchers can adopt to reduce its impact on study findings. By the end of this workshop, attendees will be equipped to incorporate placebo control measures in their experimental designs."
14,AI in Your Mind: Counterbalancing Perceived Agency and Experience in Human-AI Interaction,2022,10.1145/3491101.3519833,"In this mixed-methods study, we attempt to capture users’ conception of AI through the two-dimensional mind perception framework (perceived agency vs. experience) in cognitive psychology [13] and a series of drawing tasks. Our data illustrate how participants perceive AI entities with physical embodiment, depicting AI through devices, imaginary human figures, or full techno-ecosystems. Furthermore, we apply users’ mind maps of AI entities to highlight risks in human-AI interaction (HAII) and propose design solutions accordingly. We posit HAII research and development should be cognizant that users possess existing AI images and should exploit them as starting points for design improvement."
15,Guidelines for Human-AI Interaction,2019,10.1145/3290605.3300233,"Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles."
16,AI as a Child of Mother Earth: Regrounding Human-AI Interaction in Ecological Thinking,2024,10.1145/3613905.3644065,"The anthropocentric cultural idea that humans are active agents exerting control over their environments has been largely normalized and inscribed in practices, policies, and products of contemporary industrialized societies. This view underlies a human-ecology relationship based on resource and knowledge extraction. To create a more sustainable and equitable future, it is essential to consider alternative cultural ideas rooted in ecological thinking. This perspective underscores the interconnectedness between humans and more-than-human worlds. We propose a path to reshape the human-ecology relationship by advocating for alternative human-AI interactions. In this paper, we undertake a critical comparison between anthropocentrism and ecological thinking, using storytelling to illustrate various human-AI interactions that embody ecological thinking. We also delineate a set of design principles aimed at guiding AI developments toward fostering a more caring human-ecology relationship."
17,Playing with Dezgo: Adapting Human-AI Interaction to the Context of Play,2023,10.1145/3582437.3587198,"Play is a valuable context for investigating how users understand AI systems. This paper offers a case study that explores how users’ interactions with AI in other domains can be integrated into a game. Through this transformation, researchers and practitioners may uncover players’ misconceptions and learn how they evolve over time. In our case study, we adapt Stable Diffusion for use in a social human-AI interaction (HAI) game. The game introduces a challenging twist to a typical HAI and promotes discussion of users’ mental models with each other. We offer design considerations to help researchers design playful and social interactions that yield insights into players’ mental models."
18,Modelling Trust in Human-AI Interaction,2021,10.5555/3463952.3464253,"Trust is an important element of any interaction, but especially when we are interacting with a piece of technology which does not think like we do. Therefore, AI systems need to understand how humans trust them, and what to do to promote appropriate trust. The aim of this research is to study trust through both a formal and social lens. We will be working on formal models of trust, but with a focus on the social nature of trust in order to represent how humans trust AI. We will then employ methods from human-computer interaction research to study if these models work in practice, and what would eventually be necessary for systems to elicit appropriate levels of trust from their users. The context of this research will be AI agents which interact with their users to ofer personal support."
19,Interfaces for Explanations in Human-AI Interaction: Proposing a Design Evaluation Approach,2021,10.1145/3411763.3451759,"Explanations in Human-AI Interaction are communicated to human decision makers through interfaces. Yet, it is not clear what consequences the exact representation of such explanations as part of decision support systems (DSS) and working on machine learning (ML) models has on human decision making. We observe a need for research methods that allow for measuring the effect different eXplainable AI (XAI) interface designs have on people’s decision making. In this paper, we argue for adopting research approaches from decision theory for HCI research on XAI interface design. We outline how we used estimation tasks in human-grounded design research in order to introduce a method and measurement for collecting evidence on XAI interface effects. To this end, we investigated representations of LIME explanations in an estimation task online study as proof-of-concept for our proposal."
20,"Trust in Human-AI Interaction: Scoping Out Models, Measures, and Methods",2022,10.1145/3491101.3519772,"Trust has emerged as a key factor in people's interactions with AI-infused systems. Yet, little is known about what models of trust have been used and for what systems: robots, virtual characters, smart vehicles, decision aids, or others. Moreover, there is yet no known standard approach to measuring trust in AI. This scoping review maps out the state of affairs on trust in human-AI interaction (HAII) from the perspectives of models, measures, and methods. Findings suggest that trust is an important and multi-faceted topic of study within HAII contexts. However, most work is under-theorized and under-reported, generally not using established trust models and missing details about methods, especially Wizard of Oz. We offer several targets for systematic review work as well as a research agenda for combining the strengths and addressing the weaknesses of the current literature."
21,AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts,2021,10.1145/3491102.3517582,"Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by “unit-testing” sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications."
22,Societal-Scale Human-AI Interaction Design? How Hospitals and Companies are Integrating Pervasive Sensing into Mental Healthcare,2024,10.1145/3613904.3642793,"From wearable health tracking to sensor-laden cities, AI-enhanced pervasive sensing platforms promise far-reaching benefits yet also introduce societal risks. How might designers of these platforms effectively navigate their complex ecology and sociotechnical dynamics? To explore this question, we interviewed designers building mental health technologies who undertook this challenge. They are hospital chief medical information officers and startup founders together striving to create new sensors/AI platforms and integrate them into the healthcare ecosystem. We found that, while all designers aspired to build comprehensive care platforms, their efforts focused on serving either consumers or physicians, delivering a subset of healthcare interventions, and demonstrating system effectiveness one metric at a time. Consequently, breakdowns in patient journeys are emerging; societal risks loom large. We describe how the data economy, designers’ mindsets, and evaluation challenges led to these unintended design consequences. We discuss implications for designing pervasive sensing and AI platforms for social good."
23,Benefits of Human-AI Interaction for Expert Users Interacting with Prediction Models: a Study on Marathon Running,2024,10.1145/3640543.3645205,"Users with large domain knowledge can be reluctant to use prediction models. This also applies to the sports domain, where running coaches rarely rely on marathon prediction tools for race-plan advice for their runners’ next marathon. This paper studies the effect of adding interactivity to such prediction models, to incorporate and acknowledge users’ domain knowledge. In think-aloud sessions and an online study, we tested an interactive machine learning tool that allowed coaches to indicate the importance of earlier races feeding into the model. Our results show that coaches deploy rich knowledge when working with the model on runners familiar to them, and their adaptations improved model accuracy. Those coaches who could interact with the model displayed more trust and acceptance in the resulting predictions."
24,Effects of Communication Directionality and AI Agent Differences in Human-AI Interaction,2021,10.1145/3411764.3445256,"In Human-AI collaborative settings that are inherently interactive, direction of communication plays a role in how users perceive their AI partners. In an AI-driven cooperative game with partially observable information, players (be it the AI or the human player) require their actions to be interpreted accurately by the other player to yield a successful outcome. In this paper, we investigate social perceptions of AI agents with various directions of communication in a cooperative game setting. We measure subjective social perceptions (rapport, intelligence, and likeability) of participants towards their partners when participants believe they are playing with an AI or with a human and the nature of the communication (responsiveness and leading roles). We ran a large scale study on Mechanical Turk (n=199) of this collaborative game and find significant differences in gameplay outcome and social perception across different AI agents, different directions of communication and when the agent is perceived to be an AI/Human. We find that the bias against the AI that has been demonstrated in prior studies varies with the direction of the communication and with the AI agent."
25,UX Research on Conversational Human-AI Interaction: A Literature Review of the ACM Digital Library,2022,10.1145/3491102.3501855,"Early conversational agents (CAs) focused on dyadic human-AI interaction between humans and the CAs, followed by the increasing popularity of polyadic human-AI interaction, in which CAs are designed to mediate human-human interactions. CAs for polyadic interactions are unique because they encompass hybrid social interactions, i.e., human-CA, human-to-human, and human-to-group behaviors. However, research on polyadic CAs is scattered across different fields, making it challenging to identify, compare, and accumulate existing knowledge. To promote the future design of CA systems, we conducted a literature review of ACM publications and identified a set of works that conducted UX (user experience) research. We qualitatively synthesized the effects of polyadic CAs into four aspects of human-human interactions, i.e., communication, engagement, connection, and relationship maintenance. Through a mixed-method analysis of the selected polyadic and dyadic CA studies, we developed a suite of evaluation measurements on the effects. Our findings show that designing with social boundaries, such as privacy, disclosure, and identification, is crucial for ethical polyadic CAs. Future research should also advance usability testing methods and trust-building guidelines for conversational AI."
26,Human-AI interaction and ethics of AI: how well are we following the guidelines,2022,10.1145/3565698.3565773,"Despite the benefits of AI-enabled solutions in different industrial sectors, their technology acceptance remains challenging. The acceptance of AI technologies depends on both the Human-AI (HAI) interaction and the ethics of AI. HAI interaction significantly affects the acceptance of AI-enabled solutions. Many guidelines have been developed to support HAI interaction design, including Microsoft's Guidelines for HAI interaction. On the other hand, many ethics by design guidelines were developed, such as the Ethics Guidelines for Trustworthy AI (EGTAI) developed by European Commission. However, there is less discussion about the possible relations between these two sets of guidelines for developing AI-enabled solutions. This study aims to analyze how current AI-enabled solutions comply with these two guidelines using a case study approach. To realize this aim, we conducted a co-evaluation workshop investigating how two existing AI-enabled apps, Strava and CoronaMelder, comply with these two guidelines. In this workshop, four participants with prior knowledge of designing with AI were asked to analyze the two cases by identifying whether these guidelines were met. The workshop results implied that when HAI interactions are designed according to the HAI interaction guidelines, they do not necessarily align with the EGTAI guidelines and vice versa."
27,Interpretability as a dynamic of human-AI interaction,2020,10.1145/3411286,"Rapid development in AI technologies such as computer vision have enabled the robust perception of many aspects of our world, inviting tantalizing new experience designs. Yet these technology advances also raise many fundamental design considerations, as they become embedded in real-world applications. Designers must think carefully upon the dynamic between the person and the AI system they aim to create, and how that is realized through the design of an interpretable system. In this article, we reflect upon interpretability as a dynamic of human-AI interaction through sharing our design journey of developing an AI-enabled experience that provides ongoing information about people in the immediate vicinity for people who are blind or have low vision [1]. Through the narrative describing our design journey, we capture key insights on two themes. We begin with a discussion, central to any conversation about human-AI interaction, of how we positioned the AI system in relationship to the person using it. Here we illustrate how AI can become a resource for enabling people to extend their capabilities, standing in contrast to the automation of experiences, or systems that attempt to emulate human ability. With the human-AI partnership articulated, we then consider how it is supported through designing for interpretability. We focus on how this can be achieved by taking advantage of continuous interactions with a dynamic AI system. We then pull these insights together into a set of guidelines for designing future AI-enabled experiences."
28,Forward Reasoning Decision Support: Toward a More Complete View of the Human-AI Interaction Design Space,2021,10.1145/3464385.3464696,"Decision support systems based on AI are usually designed to generate complete outputs entirely automatically and to explain those to users. However, explanations, no matter how well designed, might not adequately address the output uncertainty of such systems in many applications. This is especially the case when the human-out-of-the-loop problem persists, which is a fundamental human limitation. There is no reason to limit decision support systems to such backward reasoning designs, though. We argue how more interactive forward reasoning designs where users are actively involved in the task can be effective in managing output uncertainty. We therefore call for a more complete view of the design space for decision support systems that includes both backward and forward reasoning designs. We argue that such a more complete view is necessary to overcome the barriers that hinder AI deployment especially in high-stakes applications."
29,"Understanding User Perceptions, Collaborative Experience and User Engagement in Different Human-AI Interaction Designs for Co-Creative Systems",2022,10.1145/3527927.3532789,"Human-AI co-creativity involves humans and AI collaborating on a shared creative product as partners. In a creative collaboration, communication is an essential component among collaborators. In many existing co-creative systems, users can communicate with the AI, usually using buttons or sliders. Typically, the AI in co-creative systems cannot communicate back to humans, limiting their potential to be perceived as partners rather than just a tool. This paper presents a study with 38 participants to explore the impact of two interaction designs, with and without AI-to-human communication, on user engagement, collaborative experience and user perception of a co-creative AI. The study involves user interaction with two prototypes of a co-creative system that contributes sketches as design inspirations during a design task. The results show improved collaborative experience and user engagement with the system incorporating AI-to-human communication. Users perceive co-creative AI as more reliable, personal, and intelligent when the AI communicates to users. The findings can be used to design effective co-creative systems, and the insights can be transferred to other fields involving human-AI interaction and collaboration."
30,Towards Mutual Theory of Mind in Human-AI Interaction: How Language Reflects What Students Perceive About a Virtual Teaching Assistant,2021,10.1145/3411764.3445645,"Building conversational agents that can conduct natural and prolonged conversations has been a major technical and design challenge, especially for community-facing conversational agents. We posit Mutual Theory of Mind as a theoretical framework to design for natural long-term human-AI interactions. From this perspective, we explore a community’s perception of a question-answering conversational agent through self-reported surveys and computational linguistic approach in the context of online education. We first examine long-term temporal changes in students’ perception of Jill Watson (JW), a virtual teaching assistant deployed in an online class discussion forum. We then explore the feasibility of inferring students’ perceptions of JW through linguistic features extracted from student-JW dialogues. We find that students’ perception of JW’s anthropomorphism and intelligence changed significantly over time. Regression analyses reveal that linguistic verbosity, readability, sentiment, diversity, and adaptability reflect student perception of JW. We discuss implications for building adaptive community-facing conversational agents as long-term companions and designing towards Mutual Theory of Mind in human-AI interaction."
31,"Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design",2020,10.1145/3313831.3376301,"Artificial Intelligence (AI) plays an increasingly important role in improving HCI and user experience. Yet many challenges persist in designing and innovating valuable human-AI interactions. For example, AI systems can make unpredictable errors, and these errors damage UX and even lead to undesired societal impact. However, HCI routinely grapples with complex technologies and mitigates their unintended consequences. What makes AI different? What makes human-AI interaction appear particularly difficult to design? This paper investigates these questions. We synthesize prior research, our own design and research experience, and our observations when teaching human-AI interaction. We identify two sources of AI's distinctive design challenges: 1) uncertainty surrounding AI's capabilities, 2) AI's output complexity, spanning from simple to adaptive complex. We identify four levels of AI systems. On each level, designers encounter a different subset of the design challenges. We demonstrate how these findings reveal new insights for designers, researchers, and design tool makers in productively addressing the challenges of human-AI interaction going forward."
32,The Human in the Infinite Loop: A Case Study on Revealing and Explaining Human-AI Interaction Loop Failures,2022,10.1145/3543758.3543761,"Interactive AI systems increasingly employ a human-in-the-loop strategy. This creates new challenges for the HCI community when designing such systems. We reveal and investigate some of these challenges in a case study with an industry partner, and developed a prototype human-in-the-loop system for preference-guided 3D model processing. Two 3D artists used it in their daily work for 3 months. We found that the human-AI loop often did not converge towards a satisfactory result and designed a lab study (N=20) to investigate this further. We analyze interaction data and user feedback through the lens of theories of human judgment to explain the observed human-in-the-loop failures with two key insights: 1) optimization using preferential choices lacks mechanisms to deal with inconsistent and contradictory human judgments; 2) machine outcomes, in turn, influence future user inputs via heuristic biases and loss aversion. To mitigate these problems, we propose descriptive UI design guidelines. Our case study draws attention to challenging and practically relevant imperfections in human-AI loops that need to be considered when designing human-in-the-loop systems."
33,Towards fluid software architectures: bidirectional human-AI interaction,2021,10.1109/ASE51524.2021.9678647,"The research on engineering software applications that employ artificial intelligence (AI) and machine learning (ML) is at an all-time peak. However, most of the research in this area is focused on the interactions between humans and AI which, in turn, is predominantly concerned with either building immersive interfaces and user experiences that allow for increased telemetry or on handling AI and ML applications in production (MLOps). Nonetheless, the research on fundamental architectural differences between AI-powered applications and traditional ones did not receive its fair share of attention. To that end, we believe that a new take on the fundamental architecture of building software applications is needed. With the ever increasing prominence of content-driven AI-powered applications, it is our conviction that 1) content could be served by servers without clients requesting, 2) servers could (should) request data from clients without waiting for their requests, and 3) interfaces should dynamically adapt to updates that happen to the intelligence driving the application. Hence, in this paper, we propose the fluid architecture that facilitates the bidirectional interaction between clients and servers as well as accommodates the co-dependent evolution of interfaces and back-end intelligence in AI-powered systems."
34,How Should AI Systems Talk to Users when Collecting their Personal Information? Effects of Role Framing and Self-Referencing on Human-AI Interaction,2021,10.1145/3411764.3445415,"AI systems collect our personal information in order to provide personalized services, raising privacy concerns and making users leery. As a result, systems have begun emphasizing overt over covert collection of information by directly asking users. This poses an important question for ethical interaction design, which is dedicated to improving user experience while promoting informed decision-making: Should the interface tout the benefits of information disclosure and frame itself as a help-provider? Or, should it appear as a help-seeker? We decided to find out by creating a mockup of a news recommendation system called Mindz and conducting an online user study (N=293) with the following four variations: AI system as help seeker vs. help provider vs. both vs. neither. Data showed that even though all participants received the same recommendations, power users tended to trust a help-seeking Mindz more whereas non-power users favored one that is both help-seeker and help-provider."
35,Human-AI Interaction in Human Resource Management: Understanding Why Employees Resist Algorithmic Evaluation at Workplaces and How to Mitigate Burdens,2021,10.1145/3411764.3445304,"Recently, Artificial Intelligence (AI) has been used to enable efficient decision-making in managerial and organizational contexts, ranging from employment to dismissal. However, to avoid employees’ antipathy toward AI, it is important to understand what aspects of AI employees like and/or dislike. In this paper, we aim to identify how employees perceive current human resource (HR) teams and future algorithmic management. Specifically, we explored what factors negatively influence employees’ perceptions of AI making work performance evaluations. Through in-depth interviews with 21 workers, we found that 1) employees feel six types of burdens (i.e., emotional, mental, bias, manipulation, privacy, and social) toward AI's introduction to human resource management (HRM), and that 2) these burdens could be mitigated by incorporating transparency, interpretability, and human intervention to algorithmic decision-making. Based on our findings, we present design efforts to alleviate employees’ burdens. To leverage AI for HRM in fair and trustworthy ways, we call for the HCI community to design human-AI collaboration systems with various HR stakeholders."
36,Learning Action Conditions for Automatic Behavior Tree Generation from Human Demonstrations,2024,10.1145/3610978.3640673,"The multitude of possible tasks and user preferences in real-world human-robot interaction scenarios renders pure pre-programming of robotic tasks inadequate. Recently, Behavior Trees (BTs) gained more focus as a modular internal task representation and particularly learning BTs directly from human video demonstrations offers also non-programming experts an opportunity to conveniently teach robots. However, automatically building BTs from human task demonstrations requires task constraints in form of action conditions. While existing work on automated BT generation often relies on pre-defined relevant features and heuristic condition computation, here we propose and evaluate different methods to automatically extract action pre- and post-conditions for BT generation from videos of human demonstrations. In particular, we first reduce the feature space using a correlation-based feature pre-selection, as well as a rule-based pre-selection based on a Decision Tree. Then, we select features that are relevant pre- and post-conditions for particular actions based on three different variance-based methods. We compare the different methods for feature selection and condition extraction on two pick-and-place tasks and discuss advantages and shortcomings of all methods in the context of learning BTs from human demonstrations."
37,(X)AI-SPOT: an (X)AI-Supported Production Process Optimization Tool,2024,10.1145/3640544.3645235,"We demonstrate (X)AI-SPOT - (X)AI-Supported Process Optimization Tool - that aims to encourage, facilitate, and enhance AI usage for process engineering and optimization in the production industry. Furthermore, (X)AI-SPOT seeks not to become a one-size-fits-all approach but a framework where each user archetype (Shop Floor Worker, Field Expert, and AI Expert) can receive tailored XAI functionality suited to their unique requirements. Currently, (X)AI-SPOT handles the Shop Floor Worker user archetype, with initial support for the Field Expert. We also describe our tool’s architecture w.r.t. extendibility and support of different user archetypes; we share our findings from an expert user interview and conclude with a discussion of design decisions and future work. Our application is available at http://exait.know-center.at/mv-ui."
38,Designing AI with Metaphors: Leveraging Ambiguity and Defamiliarization to Support Design Creativity,2024,10.1145/3635636.3664250,"Existing literature on designing AI technologies focuses on personifying the AI system using metaphors like ""collaborator"" or ""teammate."" However, these personifying metaphors may have problematic consequences and do not reflect the full breadth of the design space for human-AI interaction. We report the design process of, and initial results from, a participatory speculative design workshop that supports participants in designing AI systems using novel, non-human metaphors. We articulate two current design recommendations for interaction or workshop designers interested in AI and metaphors: Defamiliarization of AI may support creative interpretation of ambiguous metaphors and Explicit metaphorical thinking may scaffold participants’ use of ambiguity as a resource. We also discuss two potential results suggested by our initial data collection: Ambiguous metaphorical thinking may support groups negotiating values and The term AI may limit attempts at defamiliarization."
39,My Voice as a Daily Reminder: Self-Voice Alarm for Daily Goal Achievement,2024,10.1145/3613904.3641932,"Sticking to daily plans is essential for achieving life goals but challenging in reality. This study presents a self-voice alarm as a novel daily goal reminder. Based on the strong literature on the psychological effects of self-voice, we developed a voice alarm system that reminds users of daily tasks to support their consistent task completion. Over the course of 14 days, participants (N = 63) were asked to complete daily vocabulary tasks when reminded by an alarm (i.e., self-voice vs. other-voice vs. beep sound alarm). The self-voice alarm elicited higher alertness and uncomfortable feelings while fostering more days of task completion and repetition compared to the beep sound alarm. Both self-voice and other-voice alarms increased users’ perceived usefulness of the alarm system. Leveraging both quantitative and qualitative approaches, we provide a practical guideline for designing voice alarm systems that will foster users’ behavioral changes to achieve daily goals."
40,An Evaluation of Situational Autonomy for Human-AI Collaboration in a Shared Workspace Setting,2024,10.1145/3613904.3642564,"Designing interactions for human-AI teams (HATs) can be challenging due to an AI agent’s potential autonomy. Previous work suggests that higher autonomy does not always improve team performance, and situation-dependent autonomy adaptation might be beneficial. However, there is a lack of systematic empirical evaluations of such autonomy adaptation in human-AI interaction. Therefore, we propose a cooperative task in a simulated shared workspace to investigate effects of fixed levels of AI autonomy and situation-dependent autonomy adaptation on team performance and user satisfaction. We derive adaptation rules for AI autonomy from previous work and a pilot study. We implement these rule for our main experiment and find that team performance was best when humans collaborated with an agent adjusting its autonomy based on the situation. Additionally, users rated this agent highest in terms of perceived intelligence. From these results, we discuss the influence of varying autonomy degrees on HATs in shared workspaces."
41,Create Effective and Responsible AI User Experiences with The Human-AI Experience (HAX) Toolkit,2023,10.1145/3544549.3574191,"The HAX Toolkit (https://aka.ms/haxtoolkit) is a set of collaborative tools that helps teams working on user-facing AI plan, create, and evaluate human-AI user experiences. This course will help AI practitioners, human-AI interaction researchers, teachers, and students learn how to use the HAX Toolkit themselves and how to introduce it to others. The Toolkit is grounded in a set of Guidelines for Human-AI Interaction [1] that prescribe how AI systems should behave when interacting with people. Course attendees will explore the nuances of each guideline and learn how to use the AI patterns and examples in the HAX Design Library to apply the Guidelines. Course attendees will also learn how to guide cross-disciplinary teams in planning user-facing AI systems by using the HAX Workbook. For NLP systems, course attendees will learn to use the HAX Playbook [2] to anticipate and design for failures. The HAX Toolkit is a set of collaborative tools that helps teams working on user-facing AI plan, create, and evaluate human-AI user experiences. This course will help AI practitioners, human-AI interaction researchers, teachers, and students learn how to use the HAX Toolkit themselves and how to introduce it to others. Course attendees will learn the nuances of the Guidelines for Human-AI Interaction, how to lead cross-disciplinary teams in planning human-AI interaction using the HAX Workbook, and how to use the HAX Workbook to plan for failures of NLP systems."
42,Evaluation Tools for Human-AI Interactions Involving Older Adults with Mild Cognitive Impairments,2024,10.1145/3610977.3637474,"As artificial intelligence (AI) systems have already proven useful in human lives generally, there is an opportunity for specialized human-AI interaction (HAI) systems to support and provide care for older adults with mild cognitive impairment (MCI). However, the integration of this technology in this population must be thoughtfully designed to accommodate specific needs and limitations. This includes careful measurement of both humans and systems. We developed an evolving dataset categorizing relevant measurement tools into five groups: cognitive ability, demographics & personality, activity level, state of mind, and perceptions of the AI system. Each instance of the tool being used in the literature cataloged in the dataset is qualified in terms of how likely we would recommend using it in the domain of HAI for older adults with MCI based on contextual factors and internal reliability measures. This dataset will serve as a valuable resource for future research, aiding in the identification of promising areas and trends in AI systems for older adults with MCI as well as providing essential tools for future studies.CCS CONCEPTS• Computer systems organization → Robotics; • Human-centered computing → HCI design and evaluation methods."
43,Understanding the Dynamics in Creating Domain-Specific AI Design Guidelines: A Case Study of a Leading Digital Finance Company in South Korea,2024,10.1145/3613905.3650759,"Artificial intelligence (AI) significantly impacts the user experience (UX) across a wide range of products and services. However, many practitioners find it challenging to design the UX for AI-enhanced products. Although there are human-AI interaction design guidelines available, they are often too generic to fully address domain-specific challenges in fields such as finance and healthcare. Additionally, they sometimes overlook the constraints imposed by different working environments. In this research, we conducted a case study at one of the leading financial corporations in South Korea to develop their own AI product interaction guidelines. We interviewed 13 practitioners including bank UX practitioners, design agency UX designers, and academic UX researchers who were involved in this initiative, to gather insights into their experiences with crafting these guidelines. Our findings revealed a mixed reception towards existing AI design resources among practitioners, underscoring the necessity to incorporate elements specific to banking services, such as the company’s customer experience (CX) principles, overarching agent personas, multifaceted channels, and diverse interaction modalities. We also observed the importance of understanding the dynamics among various stakeholders within a company. We discuss the implications and design considerations for creating domain-specific resources specialized in AI interactions."
44,The Metacognitive Demands and Opportunities of Generative AI,2023,10.1145/3613904.3642902,"Generative AI (GenAI) systems offer unprecedented opportunities for transforming professional and personal work, yet present challenges around prompting, evaluating and relying on outputs, and optimizing workflows. We argue that metacognition—the psychological ability to monitor and control one’s thoughts and behavior—offers a valuable lens to understand and design for these usability challenges. Drawing on research in psychology and cognitive science, and recent GenAI user studies, we illustrate how GenAI systems impose metacognitive demands on users, requiring a high degree of metacognitive monitoring and control. We propose these demands could be addressed by integrating metacognitive support strategies into GenAI systems, and by designing GenAI systems to reduce their metacognitive demand by targeting explainability and customizability. Metacognition offers a coherent framework for understanding the usability challenges posed by GenAI, and provides novel research and design directions to advance human-AI interaction."
45,Intelligent User Interface for Decision Support in Drug Development,2023,10.1145/3581754.3584107,"Drug development is a complicated, costly, and lengthy process spanning multiple stages, which each may take multiple years. Only a few drugs reach or pass the clinical trial stage because high failure rates are common in the pharmaceutical industry. This work has been carried out in an applied research setting. Funded by industry, the work has strong requirements to develop working prototypes alongside scientists from the industry that guided the process. The goal of this project is to investigate support tools with computer intelligence methods that enable to shorten the amount of time needed during the pre-clinical stages of drug development and to reduce the associated experiment costs. As the result, three prototypes are developed addressing diverse requirements that arise at different stages of drug development in different units of an organization. To this end, three promising sub-problems were identified in a combined effort with industry experts: Drug formulation, Solvent selection, and Bio pharmaceutical literature research. For each task an interactive tool is proposed which includes novel intelligent visualizations designed to directly or indirectly reduce the amount of time invested in the associated tasks. Each tool has been developed in close cooperation with industry experts over the last three years. The connection between these prototypes is the interactive presentation of information derived from a computer-based intelligent process The next steps of the Ph.D. project are to assess the usability and effectiveness of each tool."
46,1-Hour Collaborative Learning Activity for Responsible Human-AI Design,2023,10.1145/3519936,"While relatively simple, the applied nature of this activity yields rich discussions about ambiguities and challenges surrounding the design of Human-AI systems in a relatively short amount of time (a single class period). Extensions of this assignment can be used applied to emphasize corporate responsibility and industry design incentives (see Recommendations )."
47,Intelligence as Agency,2024,10.1145/3672539.3695751,
48,Capturing Humans’ Mental Models of AI: An Item Response Theory Approach,2023,10.1145/3593013.3594111,"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction."
49,"Understanding Students’ Perspectives, Practices, and Challenges of Designing with AI in Special Schools",2023,10.1145/3629606.3629625,"As generative artificial intelligence (GenAI) develops rapidly, equipping students with basic knowledge of GenAI at the secondary school level is beneficial. However, few subjects in special schools cover AI-related topics, and it remains unclear how students with special educational needs (SEN) perceive, learn, and use AI. In this study, we documented the Artificial Intelligence Generated Content (AIGC) learning experience of seven students from two special schools who participated in a series of game design workshops. We analysed their learning process, usage, and perspectives of text-to-image GenAI through video analysis, questionnaire, and evaluation of design outputs. Our findings reveal the students' willingness to learn and utilise AI technology. This AI experience allows students, especially those who struggle with fine motor skills, to translate their ideas into digital images. Moreover, the text-to-image interaction enhances students' ability to convey their ideas in writing and develop their skills in conceptualisation. We also summarised significant human-AI interaction challenges, including difficulties with typing, Chinese and English spelling, and prompt writing. We conclude with recommendations for designing future inclusive human-AI interaction experiences for students with SEN in China."
